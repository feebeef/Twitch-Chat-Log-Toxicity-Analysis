{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kylet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kylet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk import word_tokenize, pos_tag_sents, pos_tag\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "import os, glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Functions and File Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming = PorterStemmer()\n",
    "stops = set(stopwords.words(\"english\")) \n",
    "\n",
    "def removeStopWords(row):\n",
    "    stemmed = row['stemmed']\n",
    "    cleaned = [w for w in stemmed if not w in stops]\n",
    "    cleaned = listToString(cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def tokenize(row):\n",
    "    message = row['message'].lower()\n",
    "    tokens = word_tokenize(message)\n",
    "    tok_words = [w for w in tokens if w.isalpha()]\n",
    "    return tok_words\n",
    "    \n",
    "def stem(row):\n",
    "    tokens = row['tokenized']\n",
    "    stemmed = [stemming.stem(word) for word in tokens]\n",
    "    return stemmed\n",
    "\n",
    "def listToString(lst):\n",
    "    text = ' '.join(lst)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"D:\\Programs\\Jupyter Notebooks\\Thesis RIP Scripts\\Annotations\\pew0\") # CHANGE VALUE PER BATCH\n",
    "files = []\n",
    "for file in glob.glob(\"*.csv\"):\n",
    "    print(file)\n",
    "    files.append(file)\n",
    "    \n",
    "print(files)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b842f38d0ac8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcoder1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcoder1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcoder2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcoder2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#coder3 = pd.read_excel('usanews0full_3.xlsx')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "coder1 = pd.read_csv(files[0])\n",
    "coder1.fillna(0)\n",
    "coder2 = pd.read_csv(files[1])\n",
    "coder2.fillna(0)\n",
    "#coder3 = pd.read_excel('usanews0full_3.xlsx')\n",
    "coder3 = pd.read_csv(files[2])\n",
    "coder3.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Documents Per Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dHarassment = pd.DataFrame()\n",
    "temp0 = coder1.loc[coder1['Direct Harassment'] == 1.0]\n",
    "temp1 = coder2.loc[coder2['Direct Harassment'] == 1.0]\n",
    "temp2 = coder3.loc[coder3['Direct Harassment'] == 1.0]\n",
    "\n",
    "dHarassment = pd.concat([temp0, temp1, temp2])\n",
    "dHarassment.drop(dHarassment.columns.difference(['Unnamed: 0', 'message','Direct Harassment']), 1, inplace=True)\n",
    "\n",
    "dHarassment.drop_duplicates(subset =\"message\", keep = False, inplace = True)\n",
    "dHarassment\n",
    "#duplicateRowsDF = dHarassment[dHarassment.duplicated()]\n",
    "#duplicateRowsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hSpeech = pd.DataFrame()\n",
    "temp0 = coder1.loc[coder1['Hate Speech'] == 1.0]\n",
    "temp1 = coder2.loc[coder2['Hate Speech'] == 1.0]\n",
    "temp2 = coder3.loc[coder3['Hate Speech'] == 1.0]\n",
    "\n",
    "hSpeech = pd.concat([temp0, temp1, temp2])\n",
    "hSpeech.drop(hSpeech.columns.difference(['Unnamed: 0', 'message','Hate Speech']), 1, inplace=True)\n",
    "\n",
    "hSpeech.drop_duplicates(subset =\"message\", keep = False, inplace = True)\n",
    "hSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sHarassment = pd.DataFrame()\n",
    "temp0 = coder1.loc[coder1['Sexual Harassment'] == 1.0]\n",
    "temp1 = coder2.loc[coder2['Sexual Harassment'] == 1.0]\n",
    "temp2 = coder3.loc[coder3['Sexual Harassment'] == 1.0]\n",
    "\n",
    "sHarassment = pd.concat([temp0, temp1, temp2])\n",
    "sHarassment.drop(sHarassment.columns.difference(['Unnamed: 0', 'message','Sexual Harassment']), 1, inplace=True)\n",
    "\n",
    "sHarassment.drop_duplicates(subset =\"message\", keep = False, inplace = True)\n",
    "sHarassment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trolling = pd.DataFrame()\n",
    "temp0 = coder1.loc[coder1['Trolling'] == 1.0]\n",
    "temp1 = coder2.loc[coder2['Trolling'] == 1.0]\n",
    "temp2 = coder3.loc[coder3['Trolling'] == 1.0]\n",
    "\n",
    "trolling = pd.concat([temp0, temp1, temp2])\n",
    "trolling.drop(trolling.columns.difference(['Unnamed: 0', 'message','Trolling']), 1, inplace=True)\n",
    "\n",
    "trolling.drop_duplicates(subset =\"message\", keep = False, inplace = True)\n",
    "trolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = pd.DataFrame()\n",
    "temp0 = coder1.loc[coder1['Others'] == 1.0]\n",
    "temp1 = coder2.loc[coder2['Others'] == 1.0]\n",
    "temp2 = coder3.loc[coder3['Others'] == 1.0]\n",
    "\n",
    "others = pd.concat([temp0, temp1, temp2])\n",
    "others.drop(others.columns.difference(['Unnamed: 0', 'message','Others']), 1, inplace=True)\n",
    "\n",
    "others.drop_duplicates(subset =\"message\", keep = False, inplace = True)\n",
    "others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic = pd.DataFrame()\n",
    "temp0 = coder1.loc[coder1['Toxic'] == 1.0]\n",
    "temp1 = coder2.loc[coder2['Toxic'] == 1.0]\n",
    "temp2 = coder3.loc[coder3['Toxic'] == 1.0]\n",
    "\n",
    "toxic = pd.concat([temp0, temp1, temp2])\n",
    "toxic.drop(toxic.columns.difference(['Unnamed: 0', 'message','Toxic']), 1, inplace=True)\n",
    "\n",
    "toxic.drop_duplicates(subset =\"message\", keep = False, inplace = True)\n",
    "toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Text Processing Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dHarassment['tokenized'] = dHarassment.apply(tokenize, axis=1)\n",
    "dHarassment['stemmed'] = dHarassment.apply(stem, axis=1)\n",
    "dHarassment['stop_words'] = dHarassment.apply(removeStopWords, axis=1)\n",
    "dHarassment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hSpeech['tokenized'] = hSpeech.apply(tokenize, axis=1)\n",
    "hSpeech['stemmed'] = hSpeech.apply(stem, axis=1)\n",
    "hSpeech['stop_words'] = hSpeech.apply(removeStopWords, axis=1)\n",
    "hSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sHarassment['tokenized'] = sHarassment.apply(tokenize, axis=1)\n",
    "sHarassment['stemmed'] = sHarassment.apply(stem, axis=1)\n",
    "sHarassment['stop_words'] = sHarassment.apply(removeStopWords, axis=1)\n",
    "sHarassment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trolling['tokenized'] = trolling.apply(tokenize, axis=1)\n",
    "trolling['stemmed'] = trolling.apply(stem, axis=1)\n",
    "trolling['stop_words'] = trolling.apply(removeStopWords, axis=1)\n",
    "trolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others['tokenized'] = others.apply(tokenize, axis=1)\n",
    "others['stemmed'] = others.apply(stem, axis=1)\n",
    "others['stop_words'] = others.apply(removeStopWords, axis=1)\n",
    "others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic['tokenized'] = toxic.apply(tokenize, axis=1)\n",
    "toxic['stemmed'] = toxic.apply(stem, axis=1)\n",
    "toxic['stop_words'] = toxic.apply(removeStopWords, axis=1)\n",
    "toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF On Direct Harassment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no text processing\n",
    "vectorizer = TfidfVectorizer()\n",
    "dHar = vectorizer.fit_transform(dHarassment['message'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df = pd.DataFrame(dHar.toarray(), columns=features)\n",
    "vals = df.sum(axis = 0, skipna = True)\n",
    "vals = vals.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removed\n",
    "vectorizer = TfidfVectorizer()\n",
    "dHar = vectorizer.fit_transform(dHarassment['stop_words'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df1 = pd.DataFrame(dHar.toarray(), columns=features)\n",
    "vals1 = df1.sum(axis = 0, skipna = True)\n",
    "vals1 = vals1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = vals[0:10]\n",
    "vals.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = vals1[0:10]\n",
    "vals1.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF on Hate Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "hSpe = vectorizer.fit_transform(hSpeech['message'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df = pd.DataFrame(hSpe.toarray(), columns=features)\n",
    "vals = df.sum(axis = 0, skipna = True)\n",
    "vals = vals.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removed\n",
    "vectorizer = TfidfVectorizer()\n",
    "hSpe = vectorizer.fit_transform(hSpeech['stop_words'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df1 = pd.DataFrame(hSpe.toarray(), columns=features)\n",
    "vals1 = df1.sum(axis = 0, skipna = True)\n",
    "vals1 = vals1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = vals[0:10]\n",
    "vals.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = vals1[0:10]\n",
    "vals1.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF on Sexual Harassment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "sHar = vectorizer.fit_transform(sHarassment['message'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df = pd.DataFrame(sHar.toarray(), columns=features)\n",
    "vals = df.sum(axis = 0, skipna = True)\n",
    "vals = vals.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removed\n",
    "vectorizer = TfidfVectorizer()\n",
    "sHar = vectorizer.fit_transform(sHarassment['stop_words'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df1 = pd.DataFrame(sHar.toarray(), columns=features)\n",
    "vals1 = df1.sum(axis = 0, skipna = True)\n",
    "vals1 = vals1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = vals[0:10]\n",
    "vals.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = vals1[0:10]\n",
    "vals1.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF on Trolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "troll = vectorizer.fit_transform(trolling['message'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df = pd.DataFrame(troll.toarray(), columns=features)\n",
    "vals = df.sum(axis = 0, skipna = True)\n",
    "vals = vals.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removed\n",
    "vectorizer = TfidfVectorizer()\n",
    "troll = vectorizer.fit_transform(trolling['stop_words'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df1 = pd.DataFrame(troll.toarray(), columns=features)\n",
    "vals1 = df1.sum(axis = 0, skipna = True)\n",
    "vals1 = vals1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = vals[0:10]\n",
    "vals.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = vals1[0:10]\n",
    "vals1.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF on Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "oth = vectorizer.fit_transform(others['message'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df = pd.DataFrame(oth.toarray(), columns=features)\n",
    "vals = df.sum(axis = 0, skipna = True)\n",
    "vals = vals.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removed\n",
    "vectorizer = TfidfVectorizer()\n",
    "oth = vectorizer.fit_transform(others['stop_words'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df1 = pd.DataFrame(oth.toarray(), columns=features)\n",
    "vals1 = df1.sum(axis = 0, skipna = True)\n",
    "vals1 = vals1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = vals[0:10]\n",
    "vals.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = vals1[0:10]\n",
    "vals1.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF on Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tox = vectorizer.fit_transform(toxic['message'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df = pd.DataFrame(tox.toarray(), columns=features)\n",
    "vals = df.sum(axis = 0, skipna = True)\n",
    "vals = vals.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words removed\n",
    "vectorizer = TfidfVectorizer()\n",
    "tox = vectorizer.fit_transform(toxic['stop_words'])\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "df1 = pd.DataFrame(tox.toarray(), columns=features)\n",
    "vals1 = df1.sum(axis = 0, skipna = True)\n",
    "vals1 = vals1.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = vals[0:10]\n",
    "vals.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals1 = vals1[0:10]\n",
    "vals1.plot.bar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
