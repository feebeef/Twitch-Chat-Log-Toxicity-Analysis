{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Phoebe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Phoebe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Phoebe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Phoebe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os, glob\n",
    "import csv\n",
    "from functools import reduce\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk import word_tokenize, pos_tag_sents, pos_tag\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "import os, glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "stops = set(stopwords.words(\"english\")) \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Direct Harassment\", \"Hate Speech\",\"Sexual Harassment\",\"Trolling\", \"Others\", \"Toxic\"]\n",
    "col = [\"preprocessed_1\", \"Direct Harassment\", \"Hate Speech\",\"Sexual Harassment\",\"Trolling\", \"Others\", \"Toxic\"]\n",
    "folder = \"../Annotations/CSVs/\"\n",
    "\n",
    "files = ['usanews.csv', 'foxnews0.csv', 'foxnews1.csv', 'foxnews2.csv', 'foxnews3.csv', 'nogla0.csv','pew0.csv', 'rae0_0.csv', \n",
    "         'rae0_1.csv', 'terror0_0.csv', 'terror0_1.csv']\n",
    "data = ['preprocessed_1']\n",
    "\n",
    "\n",
    "def combine_batch(files, name):\n",
    "    dfs = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(folder+file)\n",
    "        dfs.append(df)\n",
    "    combined_df = pd.DataFrame(columns= dfs[0].columns )\n",
    "    \n",
    "    for df in dfs:\n",
    "        combined_df.append(df, ignore_index = True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>message</th>\n",
       "      <th>Direct Harassment</th>\n",
       "      <th>Hate Speech</th>\n",
       "      <th>Sexual Harassment</th>\n",
       "      <th>Trolling</th>\n",
       "      <th>Others</th>\n",
       "      <th>Toxic</th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>POS</th>\n",
       "      <th>tagCounts</th>\n",
       "      <th>preprocessed_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Unnamed: 0.1, Unnamed: 0.1.1, Unnamed: 0.1.1.1, Unnamed: 0.1.1.1.1, message, Direct Harassment, Hate Speech, Sexual Harassment, Trolling, Others, Toxic, cleaned_message, POS, tagCounts, preprocessed_1]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "foxnews = ['foxnews0.csv', 'foxnews1.csv', 'foxnews2.csv', 'foxnews3.csv']\n",
    "rae = ['rae0_0.csv', 'rae0_1.csv']\n",
    "terror = ['terror0_0.csv', 'terror0_1.csv']\n",
    "\n",
    "x = combine_batch(foxnews, \"foxnews_ALL\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vect(documents):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(2,3), max_df=0.85, analyzer = 'word', min_df=1, max_features=10000, stop_words='english', lowercase=True)\n",
    "    vector = vectorizer.fit_transform(documents)\n",
    "    feature_names = np.array(vectorizer.get_feature_names())\n",
    "    return pd.DataFrame(vector.todense(), columns=feature_names), vector, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_tfidf_values_per_label(tfidf_df):\n",
    "    combined_tfidf = pd.DataFrame(columns= (list(features) + labels))\n",
    "    for label in labels:\n",
    "        combined_scores = tfidf_df[tfidf_df[label] == 1][features].sum()\n",
    "        to_append = list(combined_scores) + [0,0,0,0,0,0]\n",
    "        row = pd.Series(to_append, index = (list(features) + labels))\n",
    "        combined_tfidf = combined_tfidf.append(row, ignore_index=True)\n",
    "        combined_tfidf.loc[combined_tfidf.index[-1],label] = 1.0\n",
    "    return combined_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    df = pd.read_csv(folder + file)\n",
    "    df[labels] = df[labels].astype(\"float\").replace(r'^\\s*$', np.nan, regex=True).fillna(0).replace(np.nan,0)\n",
    "    df[data] = df[data].replace(np.nan, '', regex=True)\n",
    "    df[\"none\"] = (df[labels].max(axis=1) == 0).astype(float)\n",
    "    \n",
    "    tfidf_df, vector, features = get_tfidf_vect(df['preprocessed_1'])\n",
    "    tfidf_df[labels] = df[labels]\n",
    "    combined_tfidf = get_combined_tfidf_values_per_label(tfidf_df)\n",
    "    \n",
    "    combined_tfidf.to_csv(\"tfidf_per_label\" + file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
