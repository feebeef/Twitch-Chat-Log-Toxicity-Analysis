{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5000\n",
    "n_components = 5\n",
    "n_top_words = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_topics = ['talk.politics.mideast', 'rec.sport.hockey', 'soc.religion.christian', 'sci.crypt', 'comp.graphics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foxnews_FULL.csv', 'rae_FULL.csv', 'terror_FULL.csv', 'usanews.csv', 'pew0.csv', 'nogla0.csv', 'drdisrespect_FULL.csv', 'gaming_channels_GENRE.csv', 'news_channel_GENRE.csv']\n"
     ]
    }
   ],
   "source": [
    "labels = [\"Direct Harassment\", \"Hate Speech\",\"Sexual Harassment\",\"Trolling\", \"Others\", \"Toxic\"]\n",
    "\n",
    "batch_files = ['usanews.csv', 'foxnews0.csv', 'foxnews1.csv', 'foxnews2.csv', 'foxnews3.csv', \n",
    "                'nogla0.csv','pew0.csv', 'rae0_0.csv', \n",
    "                 'rae0_1.csv', 'terror0_0.csv', 'terror0_1.csv']\n",
    "\n",
    "vid_files = [\"foxnews_FULL.csv\", \"rae_FULL.csv\",  \"terror_FULL.csv\",\n",
    "             \"usanews.csv\" , \"pew0.csv\", \"nogla0.csv\", \"drdisrespect_FULL.csv\"]\n",
    "genre_files = [\"gaming_channels_GENRE.csv\", \"news_channel_GENRE.csv\" ]\n",
    "\n",
    "files = []\n",
    "files.extend(vid_files)\n",
    "files.extend(genre_files)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_tf(doc):\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features, stop_words='english', ngram_range=(1, 1))\n",
    "    vector = tf_vectorizer.fit_transform(doc)\n",
    "    feature_names = np.array(tf_vectorizer.get_feature_names())\n",
    "    return pd.DataFrame(vector.todense(), columns=feature_names), vector, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_folder = \"../Annotations/CSVs/\"\n",
    "df = pd.read_csv(src_folder + \"news_channel_GENRE.csv\", index_col = 0)\n",
    "df[\"words\"] = df[\"words\"].replace(np.nan, '', regex=True).apply(str)\n",
    "tf_df, vect, features = get_tf(df[\"words\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "def get_keywords(doc):\n",
    "    nmf = NMF(n_components=2,random_state=45)\n",
    "    vector = nmf.fit_transform(doc)\n",
    "    return nmf, vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf, vector = get_keywords(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 words for Topic #0\n",
      "['00', 'people', 'peoples', 'people√¢', 'pepper']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #1\n",
      "['ùó®ùó¶', '√ª≈ì', '√πÀÜ√π', '√π≈°√π', 'm√∞√ø']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(nmf.components_):\n",
    "    print(f'Top 15 words for Topic #{index}')\n",
    "    print([features[i] for i in topic.argsort()[:5]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phoebe\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3418: DtypeWarning: Columns (11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "src_folder = \"../Annotations/CSVs/\"\n",
    "df = pd.read_csv(src_folder + \"gaming_channels_GENRE.csv\", index_col = 0)\n",
    "df[\"words\"] = df[\"words\"].replace(np.nan, '', regex=True).apply(str)\n",
    "tf_df, vect, features = get_tf(df[df['Toxic'] == 1][\"cleaned_message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf, vector = get_keywords(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 words for Topic #0\n",
      "['bitch', 'blm', 'lives', 'shut', 'chat', 'death', 'dick', 'fuckers', 'floyd', 'pete']\n",
      "\n",
      "\n",
      "Top 15 words for Topic #1\n",
      "['white', 'trash', 'boys', 'shit', 'pete', 'like', 'vote', 'fucking', 'fuckin', 'racist']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index, topic in enumerate(nmf.components_):\n",
    "    print(f'Top 15 words for Topic #{index}')\n",
    "    print([features[i] for i in topic.argsort()[:10]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
